Running command: python train.py "8M" "/nashome/uglee/EnzFormer/results" 7 4 64 0.0001 50 8 --dropout_rate 0.2 --weight_decay 0.001
Model name: 8M
Output dimension: 7
Number of blocks: 4
Batch size: 64
Learning rate: 0.0001
Number of epochs: 50
Dropout_rate: 0.2
Weight Decay: 0.001
Head num for Attention: 8
OPTIMIZER: ADAM
Building Dataset from /nashome/uglee/EnzFormer/embed_data/8M........................
Training starts
FOLD 1/5
--------------------------------
Training epoch starts
Epoch [1/50], Training Loss: 1.6054, Validation Loss: 2.1482
Learning Rate updated to: 9.5e-05
Epoch [2/50], Training Loss: 1.2700, Validation Loss: 1.3887
Learning Rate updated to: 9.025e-05
Epoch [3/50], Training Loss: 0.9407, Validation Loss: 0.6825
Learning Rate updated to: 8.573749999999999e-05
Epoch [4/50], Training Loss: 0.6981, Validation Loss: 1.1058
Early stopping counter: 1 / 5
Learning Rate updated to: 8.145062499999998e-05
Epoch [5/50], Training Loss: 0.6284, Validation Loss: 0.7718
Early stopping counter: 2 / 5
Learning Rate updated to: 7.737809374999998e-05
Epoch [6/50], Training Loss: 0.5462, Validation Loss: 0.5992
Learning Rate updated to: 7.350918906249998e-05
Epoch [7/50], Training Loss: 0.4996, Validation Loss: 0.5067
Learning Rate updated to: 6.983372960937497e-05
Epoch [8/50], Training Loss: 0.4560, Validation Loss: 0.4805
Learning Rate updated to: 6.634204312890622e-05
Epoch [9/50], Training Loss: 0.3893, Validation Loss: 0.3577
Learning Rate updated to: 6.30249409724609e-05
Epoch [10/50], Training Loss: 0.3375, Validation Loss: 0.3328
Learning Rate updated to: 5.987369392383786e-05
Epoch [11/50], Training Loss: 0.3027, Validation Loss: 0.4165
Early stopping counter: 1 / 5
Learning Rate updated to: 5.688000922764596e-05
Epoch [12/50], Training Loss: 0.2634, Validation Loss: 0.2641
Learning Rate updated to: 5.4036008766263664e-05
Epoch [13/50], Training Loss: 0.2323, Validation Loss: 0.2518
Learning Rate updated to: 5.133420832795048e-05
Epoch [14/50], Training Loss: 0.1988, Validation Loss: 0.3947
Early stopping counter: 1 / 5
Learning Rate updated to: 4.876749791155295e-05
Epoch [15/50], Training Loss: 0.2037, Validation Loss: 0.2174
Learning Rate updated to: 4.6329123015975305e-05
Epoch [16/50], Training Loss: 0.1851, Validation Loss: 0.2045
Learning Rate updated to: 4.4012666865176535e-05
Epoch [17/50], Training Loss: 0.1435, Validation Loss: 0.2429
Early stopping counter: 1 / 5
Learning Rate updated to: 4.181203352191771e-05
Epoch [18/50], Training Loss: 0.1751, Validation Loss: 0.2625
Early stopping counter: 2 / 5
Learning Rate updated to: 3.972143184582182e-05
Epoch [19/50], Training Loss: 0.1345, Validation Loss: 0.1854
Learning Rate updated to: 3.7735360253530726e-05
Epoch [20/50], Training Loss: 0.1502, Validation Loss: 0.1987
Early stopping counter: 1 / 5
Learning Rate updated to: 3.584859224085419e-05
Epoch [21/50], Training Loss: 0.1213, Validation Loss: 0.1696
Learning Rate updated to: 3.405616262881148e-05
Epoch [22/50], Training Loss: 0.1285, Validation Loss: 0.2001
Early stopping counter: 1 / 5
Learning Rate updated to: 3.2353354497370904e-05
Epoch [23/50], Training Loss: 0.1251, Validation Loss: 0.1725
Early stopping counter: 2 / 5
Learning Rate updated to: 3.0735686772502355e-05
Epoch [24/50], Training Loss: 0.1055, Validation Loss: 0.1728
Early stopping counter: 3 / 5
Learning Rate updated to: 2.9198902433877236e-05
Epoch [25/50], Training Loss: 0.1161, Validation Loss: 0.1694
Learning Rate updated to: 2.7738957312183373e-05
Epoch [26/50], Training Loss: 0.1063, Validation Loss: 0.1457
Learning Rate updated to: 2.6352009446574204e-05
Epoch [27/50], Training Loss: 0.0966, Validation Loss: 0.1565
Early stopping counter: 1 / 5
Learning Rate updated to: 2.5034408974245492e-05
Epoch [28/50], Training Loss: 0.1035, Validation Loss: 0.1677
Early stopping counter: 2 / 5
Learning Rate updated to: 2.3782688525533216e-05
Epoch [29/50], Training Loss: 0.0962, Validation Loss: 0.1530
Early stopping counter: 3 / 5
Learning Rate updated to: 2.2593554099256555e-05
Epoch [30/50], Training Loss: 0.0841, Validation Loss: 0.1435
Learning Rate updated to: 2.1463876394293726e-05
Epoch [31/50], Training Loss: 0.0874, Validation Loss: 0.1545
Early stopping counter: 1 / 5
Learning Rate updated to: 2.039068257457904e-05
Epoch [32/50], Training Loss: 0.0896, Validation Loss: 0.1481
Early stopping counter: 2 / 5
Learning Rate updated to: 1.9371148445850086e-05
Epoch [33/50], Training Loss: 0.0798, Validation Loss: 0.1401
Learning Rate updated to: 1.840259102355758e-05
Epoch [34/50], Training Loss: 0.0816, Validation Loss: 0.1401
Learning Rate updated to: 1.74824614723797e-05
Epoch [35/50], Training Loss: 0.0831, Validation Loss: 0.1369
Learning Rate updated to: 1.6608338398760715e-05
Epoch [36/50], Training Loss: 0.0795, Validation Loss: 0.1318
Learning Rate updated to: 1.5777921478822678e-05
Epoch [37/50], Training Loss: 0.0747, Validation Loss: 0.1334
Early stopping counter: 1 / 5
Learning Rate updated to: 1.4989025404881544e-05
Epoch [38/50], Training Loss: 0.0744, Validation Loss: 0.1392
Early stopping counter: 2 / 5
Learning Rate updated to: 1.4239574134637466e-05
Epoch [39/50], Training Loss: 0.0744, Validation Loss: 0.1429
Early stopping counter: 3 / 5
Learning Rate updated to: 1.3527595427905592e-05
Epoch [40/50], Training Loss: 0.0733, Validation Loss: 0.1384
Early stopping counter: 4 / 5
Learning Rate updated to: 1.2851215656510312e-05
Epoch [41/50], Training Loss: 0.0706, Validation Loss: 0.1311
Learning Rate updated to: 1.2208654873684796e-05
Epoch [42/50], Training Loss: 0.0699, Validation Loss: 0.1282
Learning Rate updated to: 1.1598222130000555e-05
Epoch [43/50], Training Loss: 0.0699, Validation Loss: 0.1276
Learning Rate updated to: 1.1018311023500527e-05
Epoch [44/50], Training Loss: 0.0705, Validation Loss: 0.1271
Learning Rate updated to: 1.04673954723255e-05
Epoch [45/50], Training Loss: 0.0683, Validation Loss: 0.1271
Early stopping counter: 1 / 5
Learning Rate updated to: 9.944025698709225e-06
Epoch [46/50], Training Loss: 0.0675, Validation Loss: 0.1276
Early stopping counter: 2 / 5
Learning Rate updated to: 9.446824413773763e-06
Epoch [47/50], Training Loss: 0.0668, Validation Loss: 0.1280
Early stopping counter: 3 / 5
Learning Rate updated to: 8.974483193085074e-06
Epoch [48/50], Training Loss: 0.0668, Validation Loss: 0.1283
Early stopping counter: 4 / 5
Learning Rate updated to: 8.52575903343082e-06
Epoch [49/50], Training Loss: 0.0662, Validation Loss: 0.1272
Early stopping counter: 5 / 5
Early stopping triggered due to lack of improvement.
Cross-validation complete for fold 1. Validation Loss = 0.1272

